{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label :  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26f9b169710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26f9b0ce160>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Label : \", np.where(mnist.train.labels[11] == 1)[0][0])\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(mnist.train.images[11].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression (Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність : 0.9175000190734863\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])        # Х (?, 784) , ? - batch size\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))               # Weights(784, 10) \n",
    "b = tf.Variable(tf.zeros([10]))                    # Bayes(10,)\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)             # Y_pred = W * X + b\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, ([None, 10]))      # Y_train(?, 10), ? - batch size\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) # Cost Function = sum(real*log(pred))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Точність : {}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network(2 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність : 0.9743000268936157\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, ([None, 10]))\n",
    "\n",
    "W_relu = tf.Variable(tf.truncated_normal([784, 100], stddev=0.1))\n",
    "b_relu = tf.Variable(tf.truncated_normal([100], stddev=0.1))\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(x, W_relu) + b_relu)\n",
    "\n",
    "keep_probability = tf.placeholder(tf.float32)\n",
    "h_drop = tf.nn.dropout(h, keep_probability)\n",
    "\n",
    "W = tf.Variable(tf.zeros([100, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "logit = tf.matmul(h_drop, W) + b\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_probability: 0.5})\n",
    "      \n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logit, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Точність : {}'.format(sess.run(accuracy, \n",
    "                                      feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_probability: 1.})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перетворили Y в [1, 0,.., 0]\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# перетворили X в [?, 784] і нормалізували \n",
    "X_train = x_train.reshape([-1, 28*28]) / 255.\n",
    "X_test = x_test.reshape([-1, 28*28]) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(init):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_shape=(28*28,), init=init, activation='tanh'))\n",
    "    model.add(Dense(100, init=init, activation='tanh'))\n",
    "    model.add(Dense(100, init=init, activation='tanh'))\n",
    "    model.add(Dense(100, init=init, activation='tanh'))\n",
    "    model.add(Dense(10, init=init, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_shape=(784,), kernel_initializer=\"uniform\", activation=\"tanh\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"uniform\", activation=\"tanh\")`\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"uniform\", activation=\"tanh\")`\n",
      "  \"\"\"\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"uniform\", activation=\"tanh\")`\n",
      "  \n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, kernel_initializer=\"uniform\", activation=\"softmax\")`\n",
      "  import sys\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 2.3005 - acc: 0.1122 - val_loss: 2.2986 - val_acc: 0.1135\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 2.2965 - acc: 0.1124 - val_loss: 2.2928 - val_acc: 0.1135\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 2.2773 - acc: 0.1544 - val_loss: 2.2208 - val_acc: 0.2113\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 1.9185 - acc: 0.2627 - val_loss: 1.6246 - val_acc: 0.4029\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 1.2360 - acc: 0.5756 - val_loss: 0.9250 - val_acc: 0.6884\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.7649 - acc: 0.7620 - val_loss: 0.6305 - val_acc: 0.8185\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.5765 - acc: 0.8361 - val_loss: 0.5049 - val_acc: 0.8600\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.4797 - acc: 0.8655 - val_loss: 0.4358 - val_acc: 0.8790\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.4155 - acc: 0.8854 - val_loss: 0.3838 - val_acc: 0.8956\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.3702 - acc: 0.8975 - val_loss: 0.3531 - val_acc: 0.9050\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.3344 - acc: 0.9085 - val_loss: 0.3196 - val_acc: 0.9113\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.3029 - acc: 0.9162 - val_loss: 0.2953 - val_acc: 0.9186\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.2745 - acc: 0.9231 - val_loss: 0.2649 - val_acc: 0.9268\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.2492 - acc: 0.9304 - val_loss: 0.2412 - val_acc: 0.9322\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.2258 - acc: 0.9367 - val_loss: 0.2231 - val_acc: 0.9360\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.2055 - acc: 0.9421 - val_loss: 0.2091 - val_acc: 0.9414\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.1885 - acc: 0.9469 - val_loss: 0.1955 - val_acc: 0.9444\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.1748 - acc: 0.9503 - val_loss: 0.1814 - val_acc: 0.9486\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.1621 - acc: 0.9539 - val_loss: 0.1818 - val_acc: 0.9476\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.1514 - acc: 0.9567 - val_loss: 0.1657 - val_acc: 0.9505\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.1426 - acc: 0.9600 - val_loss: 0.1656 - val_acc: 0.9521\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.1344 - acc: 0.9619 - val_loss: 0.1523 - val_acc: 0.9561\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.1278 - acc: 0.9641 - val_loss: 0.1424 - val_acc: 0.9591\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.1213 - acc: 0.9661 - val_loss: 0.1546 - val_acc: 0.9533\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.1152 - acc: 0.9674 - val_loss: 0.1410 - val_acc: 0.9594\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.1102 - acc: 0.9684 - val_loss: 0.1462 - val_acc: 0.9559\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.1055 - acc: 0.9701 - val_loss: 0.1329 - val_acc: 0.9606\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.1008 - acc: 0.9712 - val_loss: 0.1307 - val_acc: 0.9605\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0969 - acc: 0.9724 - val_loss: 0.1300 - val_acc: 0.9609\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0929 - acc: 0.9739 - val_loss: 0.1306 - val_acc: 0.9599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26f98929f28>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_model = create_model('uniform')\n",
    "uniform_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "uniform_model.fit(X_train, Y_train, batch_size=64, nb_epoch=30, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_shape=(784,), kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  \"\"\"\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  \n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, kernel_initializer=\"glorot_normal\", activation=\"softmax\")`\n",
      "  import sys\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.7558 - acc: 0.8164 - val_loss: 0.3846 - val_acc: 0.8990\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.3461 - acc: 0.9042 - val_loss: 0.3026 - val_acc: 0.9165\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.2903 - acc: 0.9167 - val_loss: 0.2655 - val_acc: 0.9254\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.2595 - acc: 0.9249 - val_loss: 0.2472 - val_acc: 0.9320\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.2359 - acc: 0.9319 - val_loss: 0.2232 - val_acc: 0.9352\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.2159 - acc: 0.9378 - val_loss: 0.2069 - val_acc: 0.9397\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.1988 - acc: 0.9426 - val_loss: 0.1915 - val_acc: 0.9444\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.1837 - acc: 0.9473 - val_loss: 0.1782 - val_acc: 0.9474\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.1703 - acc: 0.9500 - val_loss: 0.1670 - val_acc: 0.9504\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.1584 - acc: 0.9543 - val_loss: 0.1567 - val_acc: 0.9541\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.1476 - acc: 0.9575 - val_loss: 0.1507 - val_acc: 0.9574\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.1383 - acc: 0.9601 - val_loss: 0.1420 - val_acc: 0.9579\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.1300 - acc: 0.9626 - val_loss: 0.1371 - val_acc: 0.9594\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.1228 - acc: 0.9647 - val_loss: 0.1284 - val_acc: 0.9626\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.1159 - acc: 0.9666 - val_loss: 0.1244 - val_acc: 0.9639\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.1097 - acc: 0.9689 - val_loss: 0.1191 - val_acc: 0.9648\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.1043 - acc: 0.9700 - val_loss: 0.1144 - val_acc: 0.9668\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0990 - acc: 0.9716 - val_loss: 0.1183 - val_acc: 0.9641\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0945 - acc: 0.9729 - val_loss: 0.1141 - val_acc: 0.9656\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0901 - acc: 0.9740 - val_loss: 0.1076 - val_acc: 0.9680\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0859 - acc: 0.9751 - val_loss: 0.1068 - val_acc: 0.9667\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0823 - acc: 0.9762 - val_loss: 0.1015 - val_acc: 0.9692\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.0786 - acc: 0.9772 - val_loss: 0.1000 - val_acc: 0.9694\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.0752 - acc: 0.9786 - val_loss: 0.0980 - val_acc: 0.9691\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0720 - acc: 0.9797 - val_loss: 0.0974 - val_acc: 0.9699\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0690 - acc: 0.9804 - val_loss: 0.0938 - val_acc: 0.9702\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0661 - acc: 0.9813 - val_loss: 0.0927 - val_acc: 0.9724\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0635 - acc: 0.9820 - val_loss: 0.0906 - val_acc: 0.9708\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0609 - acc: 0.9829 - val_loss: 0.0904 - val_acc: 0.9734\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0584 - acc: 0.9836 - val_loss: 0.0875 - val_acc: 0.9728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26f98929be0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для симетричних функцій активації(tanh) - ініціалізація ваг по методу Ксавьє (glorot_normal, glorot_uniform)\n",
    "# для несиметричних(RelU) - ініціалізація ваг по методу Хе(he_normal, he_uniform)\n",
    "glorot_model = create_model('glorot_normal')\n",
    "glorot_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "glorot_model.fit(X_train, Y_train, batch_size=64, nb_epoch=30, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layer(tensor, input_size, out_size):\n",
    "    W = tf.Variable(tf.truncated_normal([input_size, out_size], stddev=0.1))\n",
    "    b = tf.Variable(tf.truncated_normal([out_size], stddev=0.1))\n",
    "    return tf.nn.tanh(tf.matmul(tensor, W) + b)\n",
    "\n",
    "def batchnorm_layer(tensor, size):\n",
    "    batch_mean, batch_var = tf.nn.moments(tensor, [0])\n",
    "    beta = tf.Variable(tf.zeros([size]))\n",
    "    scale = tf.Variable(tf.ones([size]))\n",
    "    return tf.nn.batch_normalization(tensor, batch_mean, batch_var, beta, scale, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність : 0.9772999882698059\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "h1 = fully_connected_layer(x, 784, 100)\n",
    "h1_bn = batchnorm_layer(h1, 100)\n",
    "\n",
    "h2 = fully_connected_layer(h1_bn, 100, 100)\n",
    "y_logit = fully_connected_layer(h2, 100, 10)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_)\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_op, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y_logit, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Точність : {}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch normalization + AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність : 0.9779000282287598\n",
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "h1 = fully_connected_layer(x, 784, 100)\n",
    "h1_bn = batchnorm_layer(h1, 100)\n",
    "\n",
    "h2 = fully_connected_layer(h1_bn, 100, 100)\n",
    "y_logit = fully_connected_layer(h2, 100, 10)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_)\n",
    "train_adam_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_adam_op, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y_logit, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Точність : {}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch normalization + AdamOptimizer + Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність : 0.9793000221252441\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W_l1 = tf.get_variable(name='W_l1', shape=[784, 100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_l1 = tf.get_variable(name='b_l1', shape=[100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "h1 = tf.nn.tanh(tf.matmul(x, W_l1) + b_l1)\n",
    "\n",
    "h1_bn = batchnorm_layer(h1, 100)\n",
    "\n",
    "W_l2 = tf.get_variable(name='W_l2', shape=[100, 100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_l2 = tf.get_variable(name='b_l2', shape=[100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "h2 = tf.nn.tanh(tf.matmul(h1_bn, W_l2) + b_l2)\n",
    "\n",
    "W_l3 = tf.get_variable(name='W_l3', shape=[100, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_l3 = tf.get_variable(name='b_l3', shape=[10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "y_logit = tf.nn.tanh(tf.matmul(h2, W_l3) + b_l3)\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_)\n",
    "train_adam_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_adam_op, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y_logit, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Точність : {}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
